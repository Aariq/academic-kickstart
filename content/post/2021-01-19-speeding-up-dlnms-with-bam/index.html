---
title: Speeding up DLNMs with bam()
author: Eric R. Scott
date: '2021-01-19'
slug: dlnm-bam
categories: []
tags: [DLNMs, GAMs, R]
subtitle: ''
summary: ''
authors: []
lastmod: '2021-01-19T14:04:12-05:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


{{% alert note %}}
<p>This is part of series about distributed lag non-linear models. Please read the <a href="/post/dlnm">first post</a> for an introduction and a disclaimer.</p>
{{% /alert %}}
<p>DLNMs themselves may not be <em>that</em> computationally expensive, but when combined with random effects and other smoothers, and a large-ish dataset, I’ve noticed <code>gam()</code> being painfully slow. “Slow” is of course relative, and I’m really only talking like a couple minutes for a model to run.</p>
<p><code>bam()</code> in the <code>mgcv</code> package promises to speed up fitting and predicting for GAMs on big datasets by taking advantage of parallellization through the <code>parallel</code> package. I’m going to try to get that working and see how much it really speeds things up.</p>
<pre class="r"><code>library(mgcv)
library(dlnm)
library(parallel)
library(tictoc) #for simple benchmarking</code></pre>
<div id="standard-gam-dlnm" class="section level2">
<h2>Standard gam() DLNM</h2>
<p>This is like the DLNM I’ve been fitting for the last couple of blog posts except now the size covariate is fit as a smooth (<code>s(log_size)</code>) and there is a random effect of plot.</p>
<pre class="r"><code>tic()
growth &lt;-
  gam(log_size_next ~ 
        s(log_size) +
        s(plot, bs = &quot;re&quot;) + #random effect
        s(spei_history, L, #crossbasis function
          bs = &quot;cb&quot;, 
          k = c(3, 24), 
          xt = list(bs = &quot;cr&quot;)),
      family = gaussian(link = &quot;identity&quot;),
      method = &quot;REML&quot;,
      data = ha)
toc()</code></pre>
<pre><code>## 0.674 sec elapsed</code></pre>
<p>Remember, this is just a subset of the dataset I’m working with. This same model with the full dataset takes about 90 seconds to run, and if I add a second covariate of year, it takes about 380 seconds.</p>
</div>
<div id="set-up-parallization" class="section level2">
<h2>Set up parallization</h2>
<p><code>parallel</code> works by running code on multiple R sessions simultaneously. Read the documentation before messing with this, because I think if you set the number of clusters too high, you will crash your computer.</p>
<pre class="r"><code>cl &lt;- makeForkCluster()</code></pre>
<p>Now, I think all I have to do is re-run the same model, just with <code>bam()</code> instead of <code>gam()</code>, and include the <code>cluster</code> argument.</p>
<pre class="r"><code>tic()
growth_bam &lt;-
  bam(log_size_next ~ 
        s(log_size) +
        s(plot, bs = &quot;re&quot;) + #random effect
        s(spei_history, L, #crossbasis function
          bs = &quot;cb&quot;, 
          k = c(3, 24), 
          xt = list(bs = &quot;cr&quot;)),
      family = gaussian(link = &quot;identity&quot;),
      method = &quot;REML&quot;,
      cluster = cl,
      data = ha)
toc()</code></pre>
<pre><code>## 2.022 sec elapsed</code></pre>
<p>Hmm.. that took <strong>longer</strong>. The help file for <code>bam()</code> seems to indicate that it might not speed things up if a computationally “expensive basis” is used. So with this small dataset, maybe it’s doing more work and taking longer?</p>
<p>When I switch to <code>bam()</code> for the model using the entire dataset (~20,000 rows), I go from 380 seconds to 41 seconds—a significant improvement!</p>
</div>
